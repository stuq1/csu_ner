{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e1d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install tokenizers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ee6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_dataset = load_dataset(\"surdan/nerel_short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5965cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602cc1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302e5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303ab80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_label_names = ['O', 'I-AGE', 'B-AGE', 'B-AWARD', 'I-AWARD', 'B-CITY', 'I-CITY', 'B-COUNTRY', 'I-COUNTRY', 'B-CRIME', 'I-CRIME', 'B-DATE', 'I-DATE', 'B-DISEASE', 'I-DISEASE', 'B-DISTRICT', 'I-DISTRICT', 'B-EVENT', 'I-EVENT', 'B-FACILITY', 'I-FACILITY', 'B-FAMILY', 'I-FAMILY', 'B-IDEOLOGY', 'I-IDEOLOGY', 'B-LANGUAGE', 'I-LAW', 'B-LAW', 'B-LOCATION', 'I-LOCATION', 'B-MONEY', 'I-MONEY', 'B-NATIONALITY', 'I-NATIONALITY', 'B-NUMBER', 'I-NUMBER', 'B-ORDINAL', 'I-ORDINAL', 'B-ORGANIZATION', 'I-ORGANIZATION', 'B-PENALTY', 'I-PENALTY', 'B-PERCENT', 'I-PERCENT', 'B-PERSON', 'I-PERSON', 'I-PRODUCT', 'B-PRODUCT', 'B-PROFESSION', 'I-PROFESSION', 'B-RELIGION', 'I-RELIGION', 'B-STATE_OR_PROVINCE', 'I-STATE_OR_PROVINCE', 'B-TIME', 'I-TIME', 'B-WORK_OF_ART', 'I-WORK_OF_ART']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06129c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "ru_tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f625ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ru_tokenize_adjust_labels(all_samples_per_split):\n",
    "    ru_tokenized_samples = ru_tokenizer.batch_encode_plus(all_samples_per_split[\"sequences\"], is_split_into_words=True)\n",
    "    total_adjusted_labels = []\n",
    "    print(len(ru_tokenized_samples[\"input_ids\"]))\n",
    "    \n",
    "    for k in range(0, len(ru_tokenized_samples[\"input_ids\"])):\n",
    "        prev_wid = -1\n",
    "        word_ids_list = ru_tokenized_samples.word_ids(batch_index=k)\n",
    "        existing_label_ids = all_samples_per_split[\"ids\"][k]\n",
    "        i = -1\n",
    "        adjusted_label_ids = []\n",
    "   \n",
    "        for wid in word_ids_list:\n",
    "            if (wid is None):\n",
    "                adjusted_label_ids.append(-100)\n",
    "            elif (wid != prev_wid):\n",
    "                i = i + 1\n",
    "                adjusted_label_ids.append(existing_label_ids[i])\n",
    "                prev_wid = wid\n",
    "            else:\n",
    "                label_name = ru_label_names[existing_label_ids[i]]\n",
    "                adjusted_label_ids.append(existing_label_ids[i])\n",
    "        \n",
    "        total_adjusted_labels.append(adjusted_label_ids)\n",
    "    \n",
    "    ru_tokenized_samples[\"labels\"] = total_adjusted_labels\n",
    "    return ru_tokenized_samples\n",
    "\n",
    "ru_tokenized_dataset = ru_dataset.map(ru_tokenize_adjust_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf42323",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_tokenized_dataset[\"test\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_data_collator = DataCollatorForTokenClassification(ru_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, pipeline, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [ru_label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [ru_label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    flattened_results = {\n",
    "        \"overall_precision\": results[\"overall_precision\"],\n",
    "        \"overall_recall\": results[\"overall_recall\"],\n",
    "        \"overall_f1\": results[\"overall_f1\"],\n",
    "        \"overall_accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "    for k in results.keys():\n",
    "        if (k not in flattened_results.keys()):\n",
    "            flattened_results[k+\"_f1\"]=results[k][\"f1\"]\n",
    "\n",
    "    return flattened_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa28d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1958845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ru_model = AutoModelForTokenClassification.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\", num_labels=len(ru_label_names))\n",
    "ru_model = AutoModelForTokenClassification.from_pretrained(\"DeepPavlov/rubert-base-cased\", num_labels=len(ru_label_names))\n",
    "ru_training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tune_bert_output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 250,\n",
    "    run_name = \"ep_10_tokenized_11\",\n",
    "    save_strategy='no'\n",
    ")\n",
    "ru_trainer = Trainer(\n",
    "    model=ru_model,\n",
    "    args=ru_training_args,\n",
    "    train_dataset=ru_tokenized_dataset[\"train\"],\n",
    "    eval_dataset=ru_tokenized_dataset[\"dev\"],\n",
    "    data_collator=ru_data_collator,\n",
    "    tokenizer=ru_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "ru_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fae9db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9677400",
   "metadata": {},
   "outputs": [],
   "source": [
    "nerpipeline = pipeline('ner', model=ru_model, tokenizer=ru_tokenizer, device=0)\n",
    "text = \"Новым послом Южной Кореи в России стал бывший посол в Камбодже Чан Хо Чжин, передает Yonhap.\"\n",
    "nerpipeline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccef71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c6cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e8e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_model.save_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308a9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8decf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = AutoModelForTokenClassification.from_pretrained(\"./model\", num_labels=len(ru_label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_predict(pred_res):\n",
    "    out_res = []\n",
    "    \n",
    "    for i in range(0, len(pred_res)):\n",
    "        word = pred_res[i][\"word\"]\n",
    "        label = ru_label_names[int(pred_res[i]['entity'][6:])]\n",
    "        \n",
    "        if (word[:2] == '##'):\n",
    "            out_res[len(out_res)-1][\"word\"] = out_res[len(out_res)-1][\"word\"] + word[2:]\n",
    "        else:\n",
    "            out_res.append({\"word\": word, \"label\": label})\n",
    "    return out_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nerpipeline = pipeline('ner', model=test_model, tokenizer=ru_tokenizer, device=0)\n",
    "test_text = \"Новым послом Южной Кореи в России стал бывший посол в Камбодже Чан Хо Чжин, передает Yonhap.\"\n",
    "parser_predict(test_nerpipeline(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf2434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84dcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
